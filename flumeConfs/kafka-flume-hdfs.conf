 a1.sources  = r1
 a1.channels = c1
 a1.sinks = s1

 a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
 # 在一批中写入通道的最大消息数
 a1.sources.r1.batchSize = 1000

 # 将批次写入通道之前的最长时间（以毫秒为单位）只要达到第一个大小和时间，就会写入批次
 a1.sources.r1.batchDurationMillis = 1000
 # 当找不到Kafka存储的偏移量时，查找Zookeeper中的偏移量并将它们提交给Kafka
 a1.sources.r1.migrateZookeeperOffsets = true
 a1.sources.r1.kafka.topics = _vencano_2018-05-30
 #  a1.sources.r1.kafka.topics.regex = ^_vencano_\d{4}(\-|\/|\.)\d{1,2}\1\d{1,2}$
 a1.sources.r1.kafka.consumer.group.id = myflume
 a1.sources.r1.kafka.consumer.timeout.ms = 100
 a1.sources.r1.kafka.auto.commit.enable = true

 a1.sources.r1.kafka.bootstrap.servers = kfk1.test.tuboshi.co:9092,kfk2.test.tuboshi.co:9092,kfk3.test.tuboshi.co:9092
 a1.sources.r1.zookeeperConnect = zk1.test.tuboshi.co:2181,zk2.test.tuboshi.co:2181,zk3.test.tuboshi.co:2181

 a1.channels.c1.type = memory
 a1.channels.c1.capacity = 10000
 a1.channels.c1.transactionCapacity = 10000



 a1.sinks.s1.type = hdfs
 a1.sinks.s1.hdfs.path = /user/hongwei/kafka/%{topic}
 # Number of seconds to wait before rolling current file (0 = never roll based on time interval)
 a1.sinks.s1.hdfs.rollInterval = 180
 #以字节为单位触发滚动的文件大小（0：根据文件大小决不滚动）
 a1.sinks.s1.hdfs.rollSize = 0
 #在刷新到HDFS之前写入文件的事件数量
 a1.sinks.s1.hdfs.batchSize = 100
 # 滚动前写入文件的事件数量（0 =从不基于事件数量滚动）
 a1.sinks.s1.hdfs.rollCount = 10000
 a1.sinks.s1.hdfs.fileType = DataStream
 a1.sinks.s1.channel = c1

 a1.sources.r1.channels = c1
 a1.sinks.s1.channel = c1